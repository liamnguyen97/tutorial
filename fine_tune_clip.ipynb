{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msi/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-19 14:48:59,141] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from transformers import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import (\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    ViTFeatureExtractor,\n",
    "    BertTokenizer,\n",
    ")\n",
    "import numpy as np\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    captions_path = \".\"\n",
    "    max_text_tokens_length = 128\n",
    "    text_backbone = 'bert-base-uncased'\n",
    "    image_backbone = 'google/vit-base-patch16-224'\n",
    "    image_path = \"./dataset/flickr30k_images\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 32\n",
    "    max_epochs = 20\n",
    "    max_bad_epochs = 9\n",
    "    patience = 3\n",
    "    factor = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dataset/flickr30k_images/results.csv\", delimiter=\"|\")\n",
    "df.columns = ['image', 'caption_number', 'caption']\n",
    "df['caption'] = df['caption'].str.lstrip()\n",
    "df['caption_number'] = df['caption_number'].str.lstrip()\n",
    "df.loc[19999, 'caption_number'] = \"4\"\n",
    "df.loc[19999, 'caption'] = \"A dog runs across the grass .\"\n",
    "ids = [id_ for id_ in range(len(df) // 5) for i in range(5)]\n",
    "df['id'] = ids\n",
    "df.to_csv(\"captions.csv\", index=False)\n",
    "df.head()\n",
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msi/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):  \n",
    "    def __init__(self,image_files, captions, processor):\n",
    "        self.image_files = image_files\n",
    "        self.captions = list(captions) \n",
    "        self.processor = processor\n",
    "    def __getitem__(self,idx):\n",
    "        caption = self.captions[idx]\n",
    "        image = Image.open(f\"{CFG.image_path}/{self.image_files[idx]}\") \n",
    "        encoded_pair = self.processor(text=[caption], images=[image], return_tensors=\"pt\", max_length=CFG.max_text_tokens_length, padding='max_length', truncation=True)\n",
    "        return encoded_pair\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor = VisionTextDualEncoderProcessor(feature_extractor, tokenizer)\n",
    "train_df, valid_df = make_train_valid_dfs()\n",
    "train_ds = CLIPDataset(train_df[\"image\"].values,train_df[\"caption\"].values, processor=processor)\n",
    "valid_ds = CLIPDataset(valid_df[\"image\"].values,valid_df[\"caption\"].values, processor=processor)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds, collate_fn=collate_fn, batch_size=CFG.batch_size)\n",
    "val_dataloader = torch.utils.data.DataLoader(valid_ds, collate_fn=collate_fn, batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, epoch, max_epochs):\n",
    "    model.train()\n",
    "    nb_batches = len(train_loader)\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))   \n",
    "    epoch_loss = 0.0\n",
    "    for i, batch in enumerate(tqdm_object):\n",
    "      outputs = model(\n",
    "          input_ids=batch['input_ids'].squeeze().to(CFG.device),\n",
    "          attention_mask=batch['attention_mask'].squeeze().to(CFG.device),\n",
    "          pixel_values=batch['pixel_values'].squeeze().to(CFG.device),\n",
    "          return_loss=True)\n",
    "      loss, logits_per_image = outputs.loss, outputs.logits_per_image  # this is the image-text similarity score\n",
    "      epoch_loss += loss.item()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      tqdm_object.set_postfix(\n",
    "          batch=\"{}/{}\".format(i+1,nb_batches),\n",
    "          train_loss=loss.item(),\n",
    "          lr=get_lr(optimizer)\n",
    "          )\n",
    "    epoch_loss = epoch_loss / nb_batches\n",
    "    return epoch_loss\n",
    "\n",
    "def valid_epoch(model, dev_loader, epoch, max_epochs):\n",
    "    model.eval()\n",
    "    nb_batches = len(dev_loader)\n",
    "    tqdm_object = tqdm(dev_loader, total=len(dev_loader))\n",
    "    epoch_loss = 0.0   \n",
    "    for i, batch in enumerate(tqdm_object):\n",
    "      outputs = model(\n",
    "          input_ids=batch['input_ids'].squeeze().to(CFG.device),\n",
    "          attention_mask=batch['attention_mask'].squeeze().to(CFG.device),\n",
    "          pixel_values=batch['pixel_values'].squeeze().to(CFG.device),\n",
    "          return_loss=True)\n",
    "      loss, logits_per_image = outputs.loss, outputs.logits_per_image  # this is the image-text similarity score\n",
    "      epoch_loss += loss.item()\n",
    "      tqdm_object.set_postfix(\n",
    "          batch=\"{}/{}\".format(i+1,nb_batches),\n",
    "          dev_loss=loss.item(),\n",
    "          )\n",
    "    epoch_loss = epoch_loss / nb_batches\n",
    "    return epoch_loss\n",
    "\n",
    "def learning_loop(model):\n",
    "    model.to(CFG.device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.)\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor)\n",
    "\n",
    "    best_dev_score = float('inf')\n",
    "    train_history = []\n",
    "    dev_history = []\n",
    "    nb_bad_epochs = 0\n",
    "\n",
    "    print(\"Learning phase\")\n",
    "    print('Used device:', CFG.device)\n",
    "    print(\"--------------\")\n",
    "    for epoch in range(1, CFG.max_epochs+1):\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d}\".format(epoch, CFG.max_epochs))\n",
    "\n",
    "        if nb_bad_epochs >= CFG.max_bad_epochs:\n",
    "            print(\"Epoch {:03d}/{:03d}: exiting training after too many bad epochs.\".format(epoch, CFG.max_epochs))\n",
    "            torch.save(model.state_dict(), \"final.pt\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            epoch_train_loss = train_epoch(model=model, train_loader=train_dataloader, optimizer=optimizer, epoch=epoch, max_epochs=CFG.max_epochs)\n",
    "            epoch_dev_score = valid_epoch(model=model, dev_loader=val_dataloader, epoch=epoch, max_epochs=CFG.max_epochs)\n",
    "\n",
    "            duration = time.time() - epoch_start_time\n",
    "\n",
    "            lr_scheduler.step(epoch_dev_score)\n",
    "\n",
    "            train_history.append(epoch_train_loss)\n",
    "            dev_history.append(epoch_dev_score)\n",
    "\n",
    "            if epoch_dev_score < best_dev_score:\n",
    "                nb_bad_epochs = 0\n",
    "                best_dev_score = epoch_dev_score\n",
    "                torch.save(model.state_dict(), \"best.pt\")\n",
    "                print(\"Finished epoch {:03d}/{:03d} - Train loss: {:.7f} - Valid loss: {:.7f} - SAVED (NEW) BEST MODEL. Duration: {:.3f} s\".format(\n",
    "                epoch, CFG.max_epochs, epoch_train_loss, epoch_dev_score, duration))\n",
    "            else:\n",
    "                nb_bad_epochs += 1\n",
    "                print(\"Finished epoch {:03d}/{:03d} - Train loss: {:.7f} - Valid loss: {:.7f} - NUMBER OF BAD EPOCH.S: {}. Duration: {:.3f} s\".format(\n",
    "                epoch, CFG.max_epochs, epoch_train_loss, epoch_dev_score, nb_bad_epochs, duration))\n",
    "\n",
    "    history = {'train':train_history,'dev':dev_history}\n",
    "    return history\n",
    "  \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "def plot_history(history):\n",
    "    train_history = history['train']\n",
    "    dev_history = history['dev']\n",
    "    plt.plot(list(range(1, len(train_history)+1)), train_history, label=\"train loss\")\n",
    "    plt.plot(list(range(1, len(dev_history)+1)), dev_history, label=\"dev loss\")\n",
    "    plt.xticks(list(range(1, len(train_history)+1)))\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = VisionTextDualEncoderModel.from_vision_text_pretrained(CFG.image_backbone, CFG.text_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning phase\n",
      "Used device: cuda\n",
      "--------------\n",
      "Epoch 001/020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [42:14<00:00,  1.57it/s, batch=3973/3973, lr=0.001, train_loss=3.43]\n",
      "100%|██████████| 994/994 [06:43<00:00,  2.46it/s, batch=994/994, dev_loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 001/020 - Train loss: 3.4663736 - Valid loss: 3.4636442 - SAVED (NEW) BEST MODEL. Duration: 2937.668 s\n",
      "Epoch 002/020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [40:57<00:00,  1.62it/s, batch=3973/3973, lr=0.001, train_loss=3.43]\n",
      "100%|██████████| 994/994 [06:02<00:00,  2.74it/s, batch=994/994, dev_loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 002/020 - Train loss: 3.4657281 - Valid loss: 3.4636442 - SAVED (NEW) BEST MODEL. Duration: 2819.360 s\n",
      "Epoch 003/020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [39:22<00:00,  1.68it/s, batch=3973/3973, lr=0.001, train_loss=3.43]\n",
      "100%|██████████| 994/994 [06:07<00:00,  2.70it/s, batch=994/994, dev_loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 003/020 - Train loss: 3.4657280 - Valid loss: 3.4636442 - NUMBER OF BAD EPOCH.S: 1. Duration: 2729.844 s\n",
      "Epoch 004/020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [39:21<00:00,  1.68it/s, batch=3973/3973, lr=0.001, train_loss=3.43]\n",
      "100%|██████████| 994/994 [05:53<00:00,  2.81it/s, batch=994/994, dev_loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 004/020 - Train loss: 3.4657282 - Valid loss: 3.4636442 - NUMBER OF BAD EPOCH.S: 2. Duration: 2715.327 s\n",
      "Epoch 005/020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [39:16<00:00,  1.69it/s, batch=3973/3973, lr=0.001, train_loss=3.43]\n",
      "100%|██████████| 994/994 [05:52<00:00,  2.82it/s, batch=994/994, dev_loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 005/020 - Train loss: 3.4657282 - Valid loss: 3.4636442 - NUMBER OF BAD EPOCH.S: 3. Duration: 2709.018 s\n",
      "Epoch 006/020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [39:12<00:00,  1.69it/s, batch=3973/3973, lr=0.0001, train_loss=3.43]\n",
      "100%|██████████| 994/994 [05:51<00:00,  2.83it/s, batch=994/994, dev_loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 006/020 - Train loss: 3.4657282 - Valid loss: 3.4636442 - NUMBER OF BAD EPOCH.S: 4. Duration: 2704.367 s\n",
      "Epoch 007/020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [39:03<00:00,  1.70it/s, batch=3973/3973, lr=0.0001, train_loss=3.43]\n",
      "100%|██████████| 994/994 [05:50<00:00,  2.83it/s, batch=994/994, dev_loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 007/020 - Train loss: 3.4657282 - Valid loss: 3.4636442 - NUMBER OF BAD EPOCH.S: 5. Duration: 2694.562 s\n",
      "Epoch 008/020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3973/3973 [39:04<00:00,  1.69it/s, batch=3973/3973, lr=0.0001, train_loss=3.43]\n",
      " 60%|█████▉    | 593/994 [03:31<02:22,  2.81it/s, batch=593/994, dev_loss=3.47]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 71\u001b[0m, in \u001b[0;36mlearning_loop\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     68\u001b[0m epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     70\u001b[0m epoch_train_loss \u001b[39m=\u001b[39m train_epoch(model\u001b[39m=\u001b[39mmodel, train_loader\u001b[39m=\u001b[39mtrain_dataloader, optimizer\u001b[39m=\u001b[39moptimizer, epoch\u001b[39m=\u001b[39mepoch, max_epochs\u001b[39m=\u001b[39mCFG\u001b[39m.\u001b[39mmax_epochs)\n\u001b[0;32m---> 71\u001b[0m epoch_dev_score \u001b[39m=\u001b[39m valid_epoch(model\u001b[39m=\u001b[39;49mmodel, dev_loader\u001b[39m=\u001b[39;49mval_dataloader, epoch\u001b[39m=\u001b[39;49mepoch, max_epochs\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mmax_epochs)\n\u001b[1;32m     73\u001b[0m duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m epoch_start_time\n\u001b[1;32m     75\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep(epoch_dev_score)\n",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mvalid_epoch\u001b[0;34m(model, dev_loader, epoch, max_epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m tqdm_object \u001b[39m=\u001b[39m tqdm(dev_loader, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dev_loader))\n\u001b[1;32m     28\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m   \n\u001b[0;32m---> 29\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm_object):\n\u001b[1;32m     30\u001b[0m   outputs \u001b[39m=\u001b[39m model(\n\u001b[1;32m     31\u001b[0m       input_ids\u001b[39m=\u001b[39mbatch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mto(CFG\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m     32\u001b[0m       attention_mask\u001b[39m=\u001b[39mbatch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mto(CFG\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m     33\u001b[0m       pixel_values\u001b[39m=\u001b[39mbatch[\u001b[39m'\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mto(CFG\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m     34\u001b[0m       return_loss\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m   loss, logits_per_image \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss, outputs\u001b[39m.\u001b[39mlogits_per_image  \u001b[39m# this is the image-text similarity score\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mCLIPDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      7\u001b[0m caption \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcaptions[idx]\n\u001b[1;32m      8\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mCFG\u001b[39m.\u001b[39mimage_path\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_files[idx]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \n\u001b[0;32m----> 9\u001b[0m encoded_pair \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessor(text\u001b[39m=\u001b[39;49m[caption], images\u001b[39m=\u001b[39;49m[image], return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_length\u001b[39m=\u001b[39;49mCFG\u001b[39m.\u001b[39;49mmax_text_tokens_length, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_pair\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py:105\u001b[0m, in \u001b[0;36mVisionTextDualEncoderProcessor.__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(text, return_tensors\u001b[39m=\u001b[39mreturn_tensors, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m images \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_processor(images, return_tensors\u001b[39m=\u001b[39;49mreturn_tensors, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m images \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     encoding[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m image_features\u001b[39m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/image_processing_utils.py:464\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    463\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py:268\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 268\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(image\u001b[39m=\u001b[39mimage, mean\u001b[39m=\u001b[39mimage_mean, std\u001b[39m=\u001b[39mimage_std) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    270\u001b[0m images \u001b[39m=\u001b[39m [to_channel_dimension_format(image, data_format) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    272\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py:268\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    265\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrescale(image\u001b[39m=\u001b[39mimage, scale\u001b[39m=\u001b[39mrescale_factor) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 268\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize(image\u001b[39m=\u001b[39;49mimage, mean\u001b[39m=\u001b[39;49mimage_mean, std\u001b[39m=\u001b[39;49mimage_std) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    270\u001b[0m images \u001b[39m=\u001b[39m [to_channel_dimension_format(image, data_format) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    272\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: images}\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/models/vit/image_processing_vit.py:179\u001b[0m, in \u001b[0;36mViTImageProcessor.normalize\u001b[0;34m(self, image, mean, std, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     image: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    159\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    160\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    Normalize an image. image = (image - image_mean) / image_std.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39m        `np.ndarray`: The normalized image.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mreturn\u001b[39;00m normalize(image, mean\u001b[39m=\u001b[39;49mmean, std\u001b[39m=\u001b[39;49mstd, data_format\u001b[39m=\u001b[39;49mdata_format, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/image_transforms.py:348\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(image, mean, std, data_format)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize\u001b[39m(\n\u001b[1;32m    328\u001b[0m     image: np\u001b[39m.\u001b[39mndarray,\n\u001b[1;32m    329\u001b[0m     mean: Union[\u001b[39mfloat\u001b[39m, Iterable[\u001b[39mfloat\u001b[39m]],\n\u001b[1;32m    330\u001b[0m     std: Union[\u001b[39mfloat\u001b[39m, Iterable[\u001b[39mfloat\u001b[39m]],\n\u001b[1;32m    331\u001b[0m     data_format: Optional[ChannelDimension] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    332\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    Normalizes `image` using the mean and standard deviation specified by `mean` and `std`.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39m            The channel dimension format of the output image. If unset, will use the inferred format from the input.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m     requires_backends(normalize, [\u001b[39m\"\u001b[39;49m\u001b[39mvision\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    350\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n\u001b[1;32m    351\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    352\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPIL.Image.Image inputs are deprecated and will be removed in v4.26.0. Please use numpy arrays instead.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    353\u001b[0m             \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/utils/import_utils.py:1012\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m   1011\u001b[0m checks \u001b[39m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[39mfor\u001b[39;00m backend \u001b[39min\u001b[39;00m backends)\n\u001b[0;32m-> 1012\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m   1013\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m   1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/utils/import_utils.py:1012\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m   1011\u001b[0m checks \u001b[39m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[39mfor\u001b[39;00m backend \u001b[39min\u001b[39;00m backends)\n\u001b[0;32m-> 1012\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m   1013\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m   1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/site-packages/transformers/utils/import_utils.py:541\u001b[0m, in \u001b[0;36mis_vision_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m _pil_available:\n\u001b[1;32m    540\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m         package_version \u001b[39m=\u001b[39m importlib_metadata\u001b[39m.\u001b[39;49mversion(\u001b[39m\"\u001b[39;49m\u001b[39mPillow\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    542\u001b[0m     \u001b[39mexcept\u001b[39;00m importlib_metadata\u001b[39m.\u001b[39mPackageNotFoundError:\n\u001b[1;32m    543\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/importlib/metadata/__init__.py:996\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mversion\u001b[39m(distribution_name):\n\u001b[1;32m    990\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \n\u001b[1;32m    992\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[39m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[39m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     \u001b[39mreturn\u001b[39;00m distribution(distribution_name)\u001b[39m.\u001b[39;49mversion\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/importlib/metadata/__init__.py:627\u001b[0m, in \u001b[0;36mDistribution.version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mversion\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    626\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the 'Version' metadata for the distribution package.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m'\u001b[39m\u001b[39mVersion\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/importlib/metadata/__init__.py:612\u001b[0m, in \u001b[0;36mDistribution.metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the parsed metadata for this Distribution.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \n\u001b[1;32m    601\u001b[0m \u001b[39mThe returned object will have keys that name the various bits of\u001b[39;00m\n\u001b[1;32m    602\u001b[0m \u001b[39mmetadata.  See PEP 566 for details.\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m text \u001b[39m=\u001b[39m (\n\u001b[1;32m    605\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_text(\u001b[39m'\u001b[39m\u001b[39mMETADATA\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    606\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_text(\u001b[39m'\u001b[39m\u001b[39mPKG-INFO\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_text(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    611\u001b[0m )\n\u001b[0;32m--> 612\u001b[0m \u001b[39mreturn\u001b[39;00m _adapters\u001b[39m.\u001b[39mMessage(email\u001b[39m.\u001b[39;49mmessage_from_string(text))\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/email/__init__.py:38\u001b[0m, in \u001b[0;36mmessage_from_string\u001b[0;34m(s, *args, **kws)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parse a string into a Message object model.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[39mOptional _class and strict are passed to the Parser constructor.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39memail\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparser\u001b[39;00m \u001b[39mimport\u001b[39;00m Parser\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m Parser(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkws)\u001b[39m.\u001b[39;49mparsestr(s)\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/email/parser.py:67\u001b[0m, in \u001b[0;36mParser.parsestr\u001b[0;34m(self, text, headersonly)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparsestr\u001b[39m(\u001b[39mself\u001b[39m, text, headersonly\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a message structure from a string.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m    Returns the root of the message structure.  Optional headersonly is a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m    the file.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse(StringIO(text), headersonly\u001b[39m=\u001b[39;49mheadersonly)\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/email/parser.py:56\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self, fp, headersonly)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m     55\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     feedparser\u001b[39m.\u001b[39;49mfeed(data)\n\u001b[1;32m     57\u001b[0m \u001b[39mreturn\u001b[39;00m feedparser\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/email/feedparser.py:176\u001b[0m, in \u001b[0;36mFeedParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Push more data into the parser.\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input\u001b[39m.\u001b[39mpush(data)\n\u001b[0;32m--> 176\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_parse()\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/email/feedparser.py:180\u001b[0m, in \u001b[0;36mFeedParser._call_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_parse\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    179\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse()\n\u001b[1;32m    181\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/email/feedparser.py:240\u001b[0m, in \u001b[0;36mFeedParser._parsegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m     headers\u001b[39m.\u001b[39mappend(line)\n\u001b[1;32m    238\u001b[0m \u001b[39m# Done with the headers, so parse them and figure out what we're\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m# supposed to see in the body of the message.\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_headers(headers)\n\u001b[1;32m    241\u001b[0m \u001b[39m# Headers-only parsing is a backwards compatibility hack, which was\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[39m# necessary in the older parser, which could raise errors.  All\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39m# remaining lines in the input are thrown into the message body.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_headersonly:\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/email/feedparser.py:488\u001b[0m, in \u001b[0;36mFeedParser._parse_headers\u001b[0;34m(self, lines)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39mif\u001b[39;00m lastheader:\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cur\u001b[39m.\u001b[39mset_raw(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mheader_source_parse(lastvalue))\n\u001b[1;32m    489\u001b[0m     lastheader, lastvalue \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, []\n\u001b[1;32m    490\u001b[0m \u001b[39m# Check for envelope header, i.e. unix-from\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/bloom_lora/lib/python3.10/email/_policybase.py:303\u001b[0m, in \u001b[0;36mCompat32.header_source_parse\u001b[0;34m(self, sourcelines)\u001b[0m\n\u001b[1;32m    301\u001b[0m name, value \u001b[39m=\u001b[39m sourcelines[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    302\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mlstrip(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(sourcelines[\u001b[39m1\u001b[39m:])\n\u001b[0;32m--> 303\u001b[0m \u001b[39mreturn\u001b[39;00m (name, value\u001b[39m.\u001b[39;49mrstrip(\u001b[39m'\u001b[39;49m\u001b[39m\\r\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = learning_loop(clip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bloom_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
